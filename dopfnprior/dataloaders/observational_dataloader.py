from typing import Any, Dict, Iterator
import torch
from torch.utils.data import DataLoader

from dopfnprior.causal_graph.graph_builder import GraphBuilder
from dopfnprior.scm.scm_builder import SCMBuilder
from dopfnprior.utils.hyperparameter_sampling import sample_parameters, build_samplers
from dopfnprior.utils.select_data import select_features


class ObservationalDataLoader(DataLoader):
    """
    This is a high level class collecting and putting together the ingredients needed for making a dataloader.
    It generates on the fly purely observational data in a format compatible with NanoTabPFN.
    """
    def __init__(self,
                 num_steps: int,
                 batch_size: int, 
                 prior_config: Dict[str, Any],
                 seed: int = 42):
        """
        Initialize a new dataloader as specified by the given configuration dicts.
        
        Parameters
        ----------
        num_steps : int
            The number of batches contained in this dataloader.
        batch_size : int
            The number of data tables contained in each batch.
        prior_config : dict
            Specifies hyperparameters for the prior.
        seed : int
            Fixes the rng so that this generator samples the same data every time.
        """
        self.seed = seed
        self.num_steps: int = num_steps
        self.batch_size: int = batch_size
        self.generator = torch.Generator()
        self.generator.manual_seed(seed)
        
        self.prior_config = prior_config
        self.graph_config = prior_config["graph_config"]
        self.scm_config = prior_config["scm_config"]
        self.dataset_config = prior_config["dataset_config"]
        
        self.graph_samplers = build_samplers(self.graph_config, "graph")
        self.scm_samplers = build_samplers(self.scm_config, "scm")
        self.dataset_samplers = build_samplers(self.dataset_config, "dataset")
        
    def __len__(self) -> int:
        """Return the number of batches contained in this dataloader."""
        return self.num_steps

    def __iter__(self) -> Iterator[Dict[str, Any]]:
        """
        Specify the iterator underlying this dataloader.
        Each sampled data batch is generated by `batch_function` below.
        """
        return iter(self.batch_function() for _ in range(self.num_steps))
    
    def batch_function(self):
        # sample graph
        graph_params = sample_parameters(self.graph_samplers, "graph", self.generator)
        graph_builder = GraphBuilder(**graph_params)
        graph = graph_builder.sample_ER_DAG(self.generator)
            
        # sample SCM
        scm_params = sample_parameters(self.scm_samplers, "scm", self.generator)
        scm_builder = SCMBuilder(graph, **scm_params)
        scm = scm_builder.build(self.generator)
            
        # sample dataset parameters
        dataset_params = sample_parameters(self.dataset_samplers, "dataset", self.generator)
        num_train_samples = dataset_params["number_train_samples_per_dataset"]
        num_test_samples = dataset_params["number_test_samples_per_dataset"]
        
        # sample data from SCM
        total_samples = num_train_samples + num_test_samples
        sample_shape = (self.batch_size, total_samples)
        scm.sample_noise(sample_shape, generator=self.generator)
        data = scm.propagate(sample_shape)
        X, y = select_features(data, dataset_params["dropout_prob"], self.generator)
            
        # aggregate data in the format required by NanoTabPFN
        full_data = {}
        full_data['x'] = X
        full_data['y'] = y.unsqueeze(-1)
        full_data['target_y'] = y.unsqueeze(-1) # required by the current NanoTabPFN train loop
        full_data['single_eval_pos'] = num_train_samples
        
        return full_data