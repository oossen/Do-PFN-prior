from nanotabpfn.model import NanoTabPFNModel


prior_config = {
    
    "dataset_config": {
        # number of train samples per dataset
        # int
        "number_train_samples_per_dataset": {
            "distribution": "discrete_uniform",
            "distribution_parameters": {"low": 10, "high": 100}
        },
        # number of test samples per dataset
        # can be fixed because architecture is agnostic to the number of test samples
        # int
        "number_test_samples_per_dataset": {  # number of test samples per dataset. Can be fixed because architecture is agnostic to the number of test samples.
            "value": 100
        },
        # probability of not including a feature generated by the SCM in the final dataset
        # float
        "dropout_prob": {
            "distribution": "uniform",
            "distribution_parameters": {"low": 0.0, "high": 0.5}
        },
    },

    "graph_config": {
        # number of nodes in the causal graph
        # each node may contain several features
        # one of these will become the target, the others (if not dropped) features of the generated data
        # int
        "num_nodes": { 
            "distribution": "discrete_uniform",
            "distribution_parameters": {"low": 3, "high": 6}
        },
        # probability that any two nodes in the causal graph are connected
        # float
        "edge_prob": {
            "distribution": "uniform",
            "distribution_parameters": {"low": 0.1, "high": 0.2}
        },
    },

    "scm_config": {    
        # probability of using XGBoost mechanism as opposed to MLP mechanism
        # float
        "xgboost_prob": {
            "value": 0.0
        },
        # number of features contained in each node
        # int
        "node_dim": {
            "value": 1
        },
        # number of hidden layers in MLP mechanisms
        # int, >= 0
        "mlp_num_hidden_layers": {
            "value": 0
        },
        # number of hidden units in each layer of an MLP mechanism
        # int
        "mlp_hidden_dim": {
            "distribution": "categorical",
            "distribution_parameters": {"choices": [8, 16, 32]}
        },
        # number of hidden layers in XGBoost mechanisms
        # 0 is probably already good
        # int, >= 0
        "xgb_num_hidden_layers": {
            "value": 0
        },
        # number of hidden layers in XGBoost mechanisms
        # int
        "xgb_hidden_dim": {
            "distribution": "categorical",
            "distribution_parameters": {"choices": [16, 32, 64]}
        },
        # number of samples from a Gaussian used to fit the random XGBoost models
        # int
        "xgb_n_training_samples": {
            "distribution": "discrete_uniform",
            "distribution_parameters": {"low": 100, "high": 500}
        },
        # whether to add noise to the XGBoost mechanisms
        # bool
        "xgb_add_noise": {
            "value": True
        },
        # the standard deviation of noise sampled at root nodes when propagating through the SCM
        # float
        "root_std": {
            "value": 1.0
        },
        # the standard deviation of noise sampled at non-root nodes when propagating through the SCM
        # float
        "non_root_std": {
            "value": 0.1
        },
        # whether to use a fast SCM implementation (as opposed to doing additional checks)
        # bool
        "scm_fast": { 
            "value": True
        },
    }
}